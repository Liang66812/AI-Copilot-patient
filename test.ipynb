{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "\n",
    "# 确保PyTorch能够检测到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as DT\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.optimize import curve_fit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pretrained LLM\n",
    "QWEN2d5_7B_INST = \"/data/liangyunfei/Qwen2.5-7B-Instruct\"\n",
    "QWEN2d5_32B_INST_AWQ = \"/home/peizhengqi/Qwen/Qwen2.5-32B-Instruct-AWQ\"\n",
    "QWEN2d5_72B_INST_AWQ = \"/data/liangyunfei/Qwen2.5-72B-Instruct-AWQ\"\n",
    "\n",
    "#model_7Bn = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "#tokenizer_7Bn = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n",
    "model_32Bn = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "tokenizer_32Bn = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer_32Bn, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_prompt1 = \"\"\"\n",
    "    You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible, while being safe. \n",
    "    Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "    Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "sys_prompt1 = '你是一个非常专业的糖尿病患者照护师，请专注于给对方提供情绪价值，输出用简短的话语'\n",
    "\n",
    "def message_to_response_via_LLM(model, tokenizer, messages, max_new_tokens=256):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.01, streamer=streamer)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response\n",
    "def message_to_response_no_stream(model, tokenizer, messages, max_new_tokens=256):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.01)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt2 = '对于空腹血糖，一般认为正常范围是3.9～6.1mmol/l。餐后血糖的正常值则有所不同，餐后1小时血糖通常在6.7-9.4mmol/l之间，最多不超过11.1mmol/l。餐后2小时血糖应小于7.8mmol/l，而餐后3小时血糖应恢复到正常水平，即小于7.8mmol/l。此外，孕妇的血糖正常值有所不同。孕妇空腹血糖不应超过5.1mmol/l，餐后1小时血糖不应超过10.0mmol/l，餐后2小时血糖不应超过8.5mmol/l。'\n",
    "format_prompt = \"如果从中可以判断血糖异常，仅仅输出‘@照护师’，否则输出‘是’\"\n",
    "\n",
    "res_messages = [\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": sys_prompt1},\n",
    "    ]\n",
    "res_messages2 = [        \n",
    "        {\"role\": \"system\", \"content\": sys_prompt2},\n",
    "    ]\n",
    "\n",
    "sys_prompt3 = ''\n",
    "format_prompt3 = \"如果对方在问某种食物是否能吃类似的问题，仅仅输出‘@照护师’，否则输出‘否’\"\n",
    "res_messages3 = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt3},\n",
    "    ]\n",
    "\n",
    "answer = ''\n",
    "answer_judge1 = ''\n",
    "answer_judge = ''\n",
    "while True:\n",
    "    query_prompt = input()#input()#\"我吃的是咖喱饭\"\n",
    "    print('患者：')\n",
    "    print(query_prompt)\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": query_prompt + format_prompt})\n",
    "    answer_judge = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages2, max_new_tokens=100)\n",
    "    print('医生：')\n",
    "    if '是' not in answer_judge:\n",
    "        print(answer_judge)\n",
    "        print('@照护师')\n",
    "\n",
    "    res_messages.append({\"role\": \"system\", \"content\": answer})\n",
    "    res_messages.append({\"role\": \"system\", \"content\": query_prompt})\n",
    "    answer = message_to_response_via_LLM(model_32Bn, tokenizer_32Bn, res_messages, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    else:\n",
    "        res_messages3.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "        res_messages3.append({\"role\": \"system\", \"content\": query_prompt + format_prompt3})\n",
    "        answer_judge3 = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages3, max_new_tokens=100)\n",
    "        if '否' not in answer_judge3:\n",
    "            print('@照护师')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt2 = '对于空腹血糖，一般认为正常范围是3.9～6.1mmol/l。餐后血糖的正常值则有所不同，餐后1小时血糖通常在6.7-9.4mmol/l之间，最多不超过11.1mmol/l。餐后2小时血糖应小于7.8mmol/l，而餐后3小时血糖应恢复到正常水平，即小于7.8mmol/l。此外，孕妇的血糖正常值有所不同。孕妇空腹血糖不应超过5.1mmol/l，餐后1小时血糖不应超过10.0mmol/l，餐后2小时血糖不应超过8.5mmol/l。'\n",
    "format_prompt = \"如果从中可以判断血糖异常，仅仅输出‘@照护师’，否则输出‘是’\"\n",
    "res_messages2 = [\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": sys_prompt2},\n",
    "    ]\n",
    "answer = ''\n",
    "answer_judge = ''\n",
    "while True:\n",
    "    \n",
    "    query_prompt = input()#input()#\"我吃的是咖喱饭\"\n",
    "    print(query_prompt)\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": query_prompt + format_prompt})\n",
    "    answer_judge = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages2, max_new_tokens=100)\n",
    "    print(answer_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt3 = ''\n",
    "format_prompt = \"如果对方在问某种食物是否能吃类似的问题，仅仅输出‘@照护师’，否则输出‘否’\"\n",
    "res_messages3 = [\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": sys_prompt2},\n",
    "    ]\n",
    "answer = ''\n",
    "while True:\n",
    "    \n",
    "    query_prompt = input()#input()#\"我吃的是咖喱饭\"\n",
    "    print(query_prompt)\n",
    "    res_messages3.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "    res_messages3.append({\"role\": \"system\", \"content\": query_prompt + format_prompt})\n",
    "    answer_judge1 = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages3, max_new_tokens=100)\n",
    "    print(answer_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt2 = '对于空腹血糖，一般认为正常范围是3.9～6.1mmol/l。餐后血糖的正常值则有所不同，餐后1小时血糖通常在6.7-9.4mmol/l之间，最多不超过11.1mmol/l。餐后2小时血糖应小于7.8mmol/l，而餐后3小时血糖应恢复到正常水平，即小于7.8mmol/l。此外，孕妇的血糖正常值有所不同。孕妇空腹血糖不应超过5.1mmol/l，餐后1小时血糖不应超过10.0mmol/l，餐后2小时血糖不应超过8.5mmol/l。'\n",
    "format_prompt = \"如果从中可以判断血糖异常，仅仅输出‘@照护师’，否则输出‘是’\"\n",
    "res_messages2 = [    \n",
    "        {\"role\": \"system\", \"content\": sys_prompt2},\n",
    "    ]\n",
    "answer = ''\n",
    "answer_judge = ''\n",
    "query_prompt = input()#input()#\"我吃的是咖喱饭\"\n",
    "print(query_prompt)\n",
    "res_messages2.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "res_messages2.append({\"role\": \"system\", \"content\": query_prompt + format_prompt})\n",
    "answer_judge = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages2, max_new_tokens=100)\n",
    "print(answer_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn, json, datetime\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    text: str\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat(query: Query):\n",
    "    sys_prompt2 = '对于空腹血糖，一般认为正常范围是3.9～6.1mmol/l。餐后血糖的正常值则有所不同，餐后1小时血糖通常在6.7-9.4mmol/l之间，最多不超过11.1mmol/l。餐后2小时血糖应小于7.8mmol/l，而餐后3小时血糖应恢复到正常水平，即小于7.8mmol/l。此外，孕妇的血糖正常值有所不同。孕妇空腹血糖不应超过5.1mmol/l，餐后1小时血糖不应超过10.0mmol/l，餐后2小时血糖不应超过8.5mmol/l。'\n",
    "    format_prompt = \"如果从中可以判断血糖异常，仅仅输出‘@照护师’，否则输出‘是’\"\n",
    "    res_messages2 = [    \n",
    "            {\"role\": \"system\", \"content\": sys_prompt2},\n",
    "        ]\n",
    "    answer = ''\n",
    "    answer_judge = ''\n",
    "    query_prompt = input()#input()#\"我吃的是咖喱饭\"\n",
    "    print(query_prompt)\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": answer_judge})\n",
    "    res_messages2.append({\"role\": \"system\", \"content\": query_prompt + format_prompt})\n",
    "    answer_judge = message_to_response_no_stream(model_32Bn, tokenizer_32Bn, res_messages2, max_new_tokens=100)\n",
    "    print(answer_judge)\n",
    "    return {\"result\": answer_judge}\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=6667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载预训练的BERT模型配置\n",
    "config = BertConfig.from_pretrained('/data/liangyunfei/bert-base-uncased')\n",
    "\n",
    "# 定义自定义的BERT模型结构\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)  # 假设这是一个回归任务，输出一个值\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]  # 获取[CLS]标记的输出\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 初始化自定义BERT模型\n",
    "model = CustomBertModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 's', 'a', 'd', 's', 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
