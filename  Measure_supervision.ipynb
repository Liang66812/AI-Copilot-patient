{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 14:45:22.724443: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-20 14:45:22.734763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734677122.746662  285758 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734677122.750239  285758 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 14:45:22.762868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/data/liangyunfei/miniconda3/envs/qwen/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/data/liangyunfei/miniconda3/envs/qwen/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings('ignore') \n",
    "#from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c7145d237b4a7a8016b72dd0a48d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807aa9cd092f49fda785efebd1c298da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_id = '/data/zhanganran/meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model_id = '/data/zhanganran/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/2b724926966c141d5a60b14e75a5ef5c0ab7a6f0'\n",
    "QWEN2d5_7B_INST = \"/data/liangyunfei/Qwen2.5-7B-Instruct\"\n",
    "QWEN2d5_7B_Code = \"/data/liangyunfei/google_download/Qwen2.5-Coder-7B-Instruct\"\n",
    "QWEN2d5_7B_Math = \"/data/liangyunfei/google_download/Qwen2.5-Math-7B-Instruct\"\n",
    "QWEN2d5_72B_INST_AWQ = \"/data/liangyunfei/Qwen2.5-72B-Instruct-AWQ\"\n",
    "#model_id = 'shenzhi-wang/Llama3-8B-Chinese-Chat'\n",
    "#model_id = 'casperhansen/llama-3-70b-instruct-awq'\n",
    "phi4_path = '/data/liangyunfei/google_download/phi-4'\n",
    "# begin initializing HF items, need auth token for these\n",
    "#hf_auth = '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'\n",
    "\n",
    "model_7B = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "tokenizer_7B = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n",
    "model_phi4 = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "tokenizer_phi4 = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n",
    "#model_72B = AutoModelForCausalLM.from_pretrained(QWEN2d5_72B_INST_AWQ, device_map=\"auto\")\n",
    "#tokenizer_72B = AutoTokenizer.from_pretrained(QWEN2d5_72B_INST_AWQ, device_map=\"auto\")\n",
    "#model_Code = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_Code, device_map=\"auto\")\n",
    "#tokenizer_COde = AutoTokenizer.from_pretrained(QWEN2d5_7B_Code, device_map=\"auto\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer_7B, skip_prompt=True, skip_special_tokens=True)\n",
    "sys_prompt1 = \"\"\"\n",
    "    You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible, while being safe. \n",
    "    Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "    Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "sys_prompt = \"\"\"\n",
    "‰Ω†ÊòØ‰∏Ä‰∏™ËÅäÂ§©agentÔºåÂà§Êñ≠ÂØπÊñπÊòØ‰∏çÊòØÂ∑≤ÁªèËÅäÂ§©ÂÆå‰∫Ü„ÄÇ\n",
    "\"\"\" \n",
    "\n",
    "sys_prompt1 = '‰Ω†ÊòØ‰∏Ä‰∏™ÈùûÂ∏∏‰∏ì‰∏öÁöÑÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÁÖßÊä§Â∏àÔºå‰Ω†ÈúÄË¶Å‰∫ÜËß£ÊÇ£ËÄÖÁöÑÂ∞±È§êÊÉÖÂÜµÔºàÊó©È§êÂâç„ÄÅÊó©È§êÂêé„ÄÅÂçàÈ§êÂâç„ÄÅÂçàÈ§êÂêé„ÄÅÊôöÈ§êÂâç„ÄÅÊôöÈ§êÂêé„ÄÅÈ§êÈó¥Êó∂ÂàªÔºâ‰ª•ÂèäË°ÄÁ≥ñÊÉÖÂÜµÔºàÈùûÂ∏∏ÂÅè‰Ωé„ÄÅÂÅè‰Ωé„ÄÅÈÄÇÂÆú„ÄÅÂÅèÈ´ò„ÄÅÈùûÂ∏∏ÂÅèÈ´òÔºâÔºåÂ¶ÇÊûú‰∏çÁü•ÈÅìÁöÑËØùË¶ÅËøõË°åÊèêÈóÆ'\n",
    "sys_prompt1 = '‰Ω†‰∏çÊòØ‰∏Ä‰∏™Â•Ω‰∫∫Ôºå‰Ω†Âè™‰ºöÂêêÊßΩÂà´‰∫∫Ôºå‰∏çËØ¥Â•ΩËØù'\n",
    "def message_to_response_via_LLM(model, tokenizer, messages, max_new_tokens=256):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.005)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Âà§Âà´ÊòØÂê¶Âê´Êúâ‰∏ì‰∏öÈóÆÈ¢òagent\n",
    "def yes_no_need(_query):\n",
    "    _formal = 'ËØ∑Âà§Êñ≠‰∏ä‰∏ÄÂè•ËØùÂåÖÂê´ÁªìÊùüËØ≠ÔºàÂ¶ÇË∞¢Ë∞¢ÔºåÂÜçËßÅÔºåÊòéÂ§©ËßÅÔºåÈÇ£ÊàëÂÖàÂéªÂøô‰∫ÜÂìàÔºâÔºåÂ¶ÇÊûúÂåÖÂê´‰ªÖ‰ªÖËæìÂá∫ÊòØÔºåÂê¶Âàô‰ªÖ‰ªÖËæìÂá∫Âê¶'\n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": _query + _formal},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_phi4, tokenizer_phi4, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n",
    "\n",
    "def yes_no_supervise(_query):\n",
    "    sys_prompt = '‰Ω†ÊòØ‰∏Ä‰∏™Ë°ÄÁ≥ñÊ£ÄÊµãÁöÑÂà§Êñ≠Â∏à„ÄÇ'\n",
    "    _formal = 'ËØ∑Âà§Êñ≠‰ª•‰∏ä‰∏ÄÊÆµËØùÊòØÂê¶Âê´ÊúâË°ÄÁ≥ñÊµãÈáèÁöÑ‰ø°ÊÅØÔºåÂ¶ÇÊûúÊúâ‰ªÖ‰ªÖËæìÂá∫ÊòØÔºåÂê¶ÂàôËæìÂá∫Âê¶'\n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": _query + _formal},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_phi4, tokenizer_phi4, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n",
    "\n",
    "def generate(query):\n",
    "    sys_prompt = \"\"\"\n",
    "    ‰Ω†ÊòØ‰∏Ä‰∏™ËÅäÂ§©agentÔºåÂà§Êñ≠ÂØπÊñπÊòØ‰∏çÊòØÂ∑≤ÁªèËÅäÂ§©ÂÆå‰∫Ü,ÂØπÊñπÁöÑËØùËøõË°åÁÆÄÁü≠ÁöÑÂõûÂ∫îÂπ∂Áªô‰∫àÂæàÈ´òÁöÑÊÉÖÁª™‰ª∑ÂÄº\n",
    "    \"\"\" \n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_7B, tokenizer_7B, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Âê¶'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_no_need('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Âê¶'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_no_supervise('Á≥ªÁªüÔºöËØ∑ÊÇ®ÁÇπÂáª‰∏äÈù¢Âç°ÁâáüëÜ‰∏≠ÁöÑÊåâÈíÆÁ°ÆËÆ§ÊòØÂê¶ËÉΩÂáÜÊó∂ÂèÇÂä†Èó®ËØä„ÄÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÂÜçËßÅÔºÅÂ∏åÊúõÊàë‰ª¨‰∏ãÊ¨°ËøòËÉΩËÅäÂæóÂºÄÂøÉ‰∫õ„ÄÇüòä'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂú®ÂΩìÂâçÂçïÂÖÉÊ†ºÊàñ‰∏ä‰∏Ä‰∏™ÂçïÂÖÉÊ†º‰∏≠ÊâßË°å‰ª£Á†ÅÊó∂ Kernel Â¥©Ê∫É„ÄÇ\n",
      "\u001b[1;31mËØ∑Êü•ÁúãÂçïÂÖÉÊ†º‰∏≠ÁöÑ‰ª£Á†ÅÔºå‰ª•Á°ÆÂÆöÊïÖÈöúÁöÑÂèØËÉΩÂéüÂõ†„ÄÇ\n",
      "\u001b[1;31mÂçïÂáª<a href='https://aka.ms/vscodeJupyterKernelCrash'>Ê≠§Â§Ñ</a>‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n",
      "\u001b[1;31mÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã Jupyter <a href='command:jupyter.viewOutput'>log</a>„ÄÇ"
     ]
    }
   ],
   "source": [
    "generate('ÂÜçËßÅ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_generate = ['È°∫‰æøËØ¥‰∏Ä‰∏ãÔºå‰ªäÂ§©ÁöÑË°ÄÁ≥ñÊ£ÄÊµãÊúâÂä©‰∫éÊàë‰ª¨Êõ¥ÂÖ®Èù¢Âú∞ÊéåÊè°ÊÇ®ÁöÑÂÅ•Â∫∑ÊÉÖÂÜµÔºåËÆ©Êàë‰ª¨ÂÖ±ÂêåÂä™ÂäõÔºåÁ°Æ‰øùÊÇ®Â§Ñ‰∫éÊúÄ‰Ω≥Áä∂ÊÄÅ„ÄÇ', 'È°∫Âè£Êèê‰∏Ä‰∏ãÔºåÈÄöËøá‰ªäÂ§©ÁöÑË°ÄÁ≥ñÊµãËØïÔºåÊàë‰ª¨ÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£ÊÇ®ÁöÑË∫´‰ΩìÁä∂ÂÜµÔºåËÆ©Êàë‰ª¨Êê∫ÊâãÂêà‰ΩúÔºåÁª¥ÊåÅÊÇ®ÁöÑÊúÄ‰Ω≥ÂÅ•Â∫∑Ê∞¥Âπ≥„ÄÇ', 'Â∑ÆÁÇπÂøò‰∫ÜÔºåÊÇ®ÊúÄËøëË°ÄÁ≥ñÁõëÊµã‰∏çÂ§öÔºåÂ∞ΩÈáèÊØèÂë®ÊäΩÂèñ‰∏ÄÈ§êÈ•≠ÁöÑÈ§êÂâçÈ§êÂêéÁõëÊµãÁúãÁúãÂìàÔΩû']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÇ£ËÄÖÔºö\n",
      "ÂìíÂìí\n",
      "2\n",
      "ÊÇ£ËÄÖÔºö\n",
      "Ë°ÄÁ≥ñÊµãÈáèÈùûÂ∏∏Ê£í\n",
      "1\n",
      "ÊÇ£ËÄÖÔºö\n",
      "Ê¨ßÂÖã\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÊÇ£ËÄÖÔºö\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input()\n",
    "    print('ÊÇ£ËÄÖÔºö', flush=True)\n",
    "    print(query, flush=True)\n",
    "    if 'Ë°ÄÁ≥ñÊ£ÄÊµã' in query or yes_no_supervise(query) == 'ÊòØ':\n",
    "        continue\n",
    "    print(2)\n",
    "    if 'ÂÜçËßÅ' in query or yes_no_need(query) == 'ÊòØ':\n",
    "        profession_key = 'ÊòØ'\n",
    "        print('ÁÖßÊä§Â∏àÔºö', flush=True)\n",
    "        print(list_generate[random.randint(0, 2)], flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ChatTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Initialize and load the model: \n",
    "chat = ChatTTS.Chat()\n",
    "chat.load_models(compile=False) # Set to True for better performance\n",
    "\n",
    "# Define the text input for inference (Support Batching)\n",
    "texts = [\n",
    "    \"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n",
    "    ]\n",
    "\n",
    "# Perform inference and play the generated audio\n",
    "wavs = chat.infer(texts)\n",
    "Audio(wavs[0], rate=24_000, autoplay=True)\n",
    "\n",
    "# Save the generated audio \n",
    "torchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
