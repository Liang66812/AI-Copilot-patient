{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 14:45:22.724443: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-20 14:45:22.734763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734677122.746662  285758 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734677122.750239  285758 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 14:45:22.762868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/data/liangyunfei/miniconda3/envs/qwen/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/data/liangyunfei/miniconda3/envs/qwen/lib/python3.10/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings('ignore') \n",
    "#from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c7145d237b4a7a8016b72dd0a48d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807aa9cd092f49fda785efebd1c298da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_id = '/data/zhanganran/meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "model_id = '/data/zhanganran/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/2b724926966c141d5a60b14e75a5ef5c0ab7a6f0'\n",
    "QWEN2d5_7B_INST = \"/data/liangyunfei/Qwen2.5-7B-Instruct\"\n",
    "QWEN2d5_7B_Code = \"/data/liangyunfei/google_download/Qwen2.5-Coder-7B-Instruct\"\n",
    "QWEN2d5_7B_Math = \"/data/liangyunfei/google_download/Qwen2.5-Math-7B-Instruct\"\n",
    "QWEN2d5_72B_INST_AWQ = \"/data/liangyunfei/Qwen2.5-72B-Instruct-AWQ\"\n",
    "#model_id = 'shenzhi-wang/Llama3-8B-Chinese-Chat'\n",
    "#model_id = 'casperhansen/llama-3-70b-instruct-awq'\n",
    "phi4_path = '/data/liangyunfei/google_download/phi-4'\n",
    "# begin initializing HF items, need auth token for these\n",
    "#hf_auth = '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'\n",
    "\n",
    "model_7B = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "tokenizer_7B = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n",
    "model_phi4 = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "tokenizer_phi4 = AutoTokenizer.from_pretrained(QWEN2d5_7B_INST, device_map=\"auto\")\n",
    "\n",
    "#model_72B = AutoModelForCausalLM.from_pretrained(QWEN2d5_72B_INST_AWQ, device_map=\"auto\")\n",
    "#tokenizer_72B = AutoTokenizer.from_pretrained(QWEN2d5_72B_INST_AWQ, device_map=\"auto\")\n",
    "#model_Code = AutoModelForCausalLM.from_pretrained(QWEN2d5_7B_Code, device_map=\"auto\")\n",
    "#tokenizer_COde = AutoTokenizer.from_pretrained(QWEN2d5_7B_Code, device_map=\"auto\")\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer_7B, skip_prompt=True, skip_special_tokens=True)\n",
    "sys_prompt1 = \"\"\"\n",
    "    You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible, while being safe. \n",
    "    Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "    Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "sys_prompt = \"\"\"\n",
    "你是一个聊天agent，判断对方是不是已经聊天完了。\n",
    "\"\"\" \n",
    "\n",
    "sys_prompt1 = '你是一个非常专业的糖尿病患者照护师，你需要了解患者的就餐情况（早餐前、早餐后、午餐前、午餐后、晚餐前、晚餐后、餐间时刻）以及血糖情况（非常偏低、偏低、适宜、偏高、非常偏高），如果不知道的话要进行提问'\n",
    "sys_prompt1 = '你不是一个好人，你只会吐槽别人，不说好话'\n",
    "def message_to_response_via_LLM(model, tokenizer, messages, max_new_tokens=256):\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.005)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判别是否含有专业问题agent\n",
    "def yes_no_need(_query):\n",
    "    _formal = '请判断上一句话包含结束语（如谢谢，再见，明天见，那我先去忙了哈），如果包含仅仅输出是，否则仅仅输出否'\n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": _query + _formal},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_phi4, tokenizer_phi4, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n",
    "\n",
    "def yes_no_supervise(_query):\n",
    "    sys_prompt = '你是一个血糖检测的判断师。'\n",
    "    _formal = '请判断以上一段话是否含有血糖测量的信息，如果有仅仅输出是，否则输出否'\n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": _query + _formal},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_phi4, tokenizer_phi4, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n",
    "\n",
    "def generate(query):\n",
    "    sys_prompt = \"\"\"\n",
    "    你是一个聊天agent，判断对方是不是已经聊天完了,对方的话进行简短的回应并给予很高的情绪价值\n",
    "    \"\"\" \n",
    "    res_messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},  # 37 62\n",
    "        ]\n",
    "    yes_no = message_to_response_via_LLM(model_7B, tokenizer_7B, res_messages, max_new_tokens=1000)\n",
    "    return yes_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'否'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_no_need('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'否'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_no_supervise('系统：请您点击上面卡片👆中的按钮确认是否能准时参加门诊。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'再见！希望我们下次还能聊得开心些。😊'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "generate('再见')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_generate = ['顺便说一下，今天的血糖检测有助于我们更全面地掌握您的健康情况，让我们共同努力，确保您处于最佳状态。', '顺口提一下，通过今天的血糖测试，我们可以更好地理解您的身体状况，让我们携手合作，维持您的最佳健康水平。', '差点忘了，您最近血糖监测不多，尽量每周抽取一餐饭的餐前餐后监测看看哈～']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "患者：\n",
      "哒哒\n",
      "2\n",
      "患者：\n",
      "血糖测量非常棒\n",
      "1\n",
      "患者：\n",
      "欧克\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m患者：\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input()\n",
    "    print('患者：', flush=True)\n",
    "    print(query, flush=True)\n",
    "    if '血糖检测' in query or yes_no_supervise(query) == '是':\n",
    "        continue\n",
    "    print(2)\n",
    "    if '再见' in query or yes_no_need(query) == '是':\n",
    "        profession_key = '是'\n",
    "        print('照护师：', flush=True)\n",
    "        print(list_generate[random.randint(0, 2)], flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import ChatTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Initialize and load the model: \n",
    "chat = ChatTTS.Chat()\n",
    "chat.load_models(compile=False) # Set to True for better performance\n",
    "\n",
    "# Define the text input for inference (Support Batching)\n",
    "texts = [\n",
    "    \"So we found being competitive and collaborative was a huge way of staying motivated towards our goals, so one person to call when you fall off, one person who gets you back on then one person to actually do the activity with.\",\n",
    "    ]\n",
    "\n",
    "# Perform inference and play the generated audio\n",
    "wavs = chat.infer(texts)\n",
    "Audio(wavs[0], rate=24_000, autoplay=True)\n",
    "\n",
    "# Save the generated audio \n",
    "torchaudio.save(\"output.wav\", torch.from_numpy(wavs[0]), 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
