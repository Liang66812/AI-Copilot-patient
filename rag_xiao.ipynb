{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/helulu/miniconda3/envs/jiuan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as DT\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.optimize import curve_fit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "#from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:06<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "\n",
    "# 模型名称\n",
    "model_name = \"/data/liangyunfei/Qwen2.5-72B-Instruct-AWQ\"\n",
    "\n",
    "# 自动分配设备\n",
    "model_72Bn = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer_72Bn = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer_72Bn.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return model_72Bn.chat(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "def get_embeddings(sentences, model_path='/home/helulu/jiuan_health/bge-large-zh-v1___5'):\n",
    "    # 加载模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # 对输入文本进行编码\n",
    "    encoded_input = tokenizer(\n",
    "        sentences, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length = 512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # 计算嵌入向量\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        # 使用 CLS token 的输出作为句子嵌入\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "    \n",
    "    # 归一化嵌入向量\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "#创建向量知识库\n",
    "class KB:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        self.docs = self.split_text(self.data)\n",
    "        self.embeddings = self.encode(self.docs)\n",
    "\n",
    "    #分词\n",
    "    def split_text(self, text):\n",
    "        chunks = [chunk.strip() for chunk in text.split('\\n') if chunk.strip()]\n",
    "        return chunks\n",
    "    \n",
    "    #创建向量\n",
    "    def encode(self,texts):\n",
    "        embeds = []\n",
    "        for text in texts:\n",
    "            response = get_embeddings(text)\n",
    "            embeds.append(response)\n",
    "        return np.array(embeds)\n",
    "\n",
    "\n",
    "kb = KB(\"/home/helulu/jiuan_health/faiss/cleaned_result.txt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "查询: 吃手抓饼可以吗？\n",
      "\n",
      "相似度最高的5个结果：\n",
      "\n",
      "1. 相似度分数: 0.6819\n",
      "文本: 手抓饼等高油脂食物，建议尽量少吃，因为高油脂食物会增加胰岛素抵抗，影响血糖控制。\n",
      "\n",
      "2. 相似度分数: 0.6799\n",
      "文本: 油脂较大的食物（如手抓饼）不仅热量高，还可能影响血糖控制，建议尽量少吃。\n",
      "\n",
      "3. 相似度分数: 0.6637\n",
      "文本: 手抓饼等加工食品通常含有较多的油脂和糖分，容易导致血糖波动。建议尽量少吃这类食物，如果需要加餐，可以选择一些低GI的食物，如全麦面包、燕麦片等。\n",
      "\n",
      "4. 相似度分数: 0.6434\n",
      "文本: 手抓饼油脂较大，建议尽量少吃，芋头和手抓饼都是主食，加起来吃拳头大小为宜，午餐需增加蔬菜，蔬菜富含丰富的膳食纤维，可以帮助延缓主食消化吸收，稳定餐后血糖，每顿最好能吃到半斤。\n",
      "\n",
      "5. 相似度分数: 0.6406\n",
      "文本: 手抓饼油脂较大，建议尽量少吃，芋头和手抓饼都是主食，加起来吃拳头大小为宜，午餐缺一份蔬菜，注意增加蔬菜，蔬菜富含丰富的膳食纤维，可以帮助延缓主食消化吸收，稳定餐后血糖，每顿最好能吃到半斤。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245832/1182251941.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  score = float(np.dot(e, kb_embed.T))\n"
     ]
    }
   ],
   "source": [
    "# 相似度匹配 - 返回前k个最相似的结果\n",
    "def search(query, kb, top_k=5):\n",
    "    # 存储所有文档的分数\n",
    "    scores = []\n",
    "    e = get_embeddings(query)\n",
    "    \n",
    "    # 计算所有文档的相似度分数\n",
    "    for i, kb_embed in enumerate(kb.embeddings):\n",
    "        score = float(np.dot(e, kb_embed.T))\n",
    "        scores.append((score, i))\n",
    "    \n",
    "    # 按分数降序排序并获取前top_k个结果\n",
    "    scores.sort(reverse=True)\n",
    "    top_results = scores[:top_k]\n",
    "    \n",
    "    # 返回结果列表\n",
    "    results = []\n",
    "    for score, idx in top_results:\n",
    "        results.append({\n",
    "            'score': score,\n",
    "            'text': kb.docs[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 使用示例\n",
    "query = \"吃手抓饼可以吗？\"\n",
    "results = search(query, kb)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"\\n查询: {query}\")\n",
    "print(\"\\n相似度最高的5个结果：\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. 相似度分数: {result['score']:.4f}\")\n",
    "    print(f\"文本: {result['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'土豆也是主食，如果吃了土豆，米饭就要减量了'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#相似度匹配\n",
    "def search(query,kb):\n",
    "    max_score = 0\n",
    "    max_index = 0\n",
    "    e = get_embeddings(query)\n",
    "    for i,kb_embed in enumerate(kb.embeddings):\n",
    "        score = np.dot(e,kb_embed.T)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_index = i\n",
    "    return kb.docs[max_index]\n",
    "\n",
    "search(\"我中午吃的米饭和土豆？\",kb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245832/1182251941.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  score = float(np.dot(e, kb_embed.T))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "土豆和大米可以一起吃，但需要注意控制总量，以避免碳水化合物摄入过多。由于土豆属于主食类食物，其淀粉含量较高，与米饭一起食用时，容易导致餐后血糖升高。因此，建议在食用土豆的同时适当减少米饭的摄入量，每餐主食总量最好不超过2两（100克），或者控制在自己拳头大小的范围内。这样既能保证营养均衡，又能避免碳水化合物摄入过量带来的健康问题。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"土豆和大米可以一起吃吗\"\n",
    "context = search(query,kb)\n",
    "prompt_template = \"\"\"\n",
    "        基于{context}，回答{query}：不要分点回答，要生成一段文字\n",
    "        \"\"\"\n",
    "prompt = prompt_template.format(context=context,query=query)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "text = tokenizer_72Bn.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer_72Bn([text], return_tensors=\"pt\").to(model_72Bn.device)\n",
    "generated_ids = model_72Bn  .generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer_72Bn.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
